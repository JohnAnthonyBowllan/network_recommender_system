{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from collections import defaultdict, Counter\n",
    "import itertools\n",
    "import json\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import plot_roc_curve, roc_curve, auc, roc_auc_score, accuracy_score, classification_report\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "stash_path = f'{cwd}/stash,'\n",
    "if not os.path.isdir(stash_path):\n",
    "    os.makedirs(stash_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLAN\n",
    "1. Take out subset of users who have more than 30 interactions to EVALUATE the business trajectories network.\n",
    "2. Build business trajectories network.\n",
    "3. For test users, generate lists of recommendations with different max direct neighbors allowed (1, ..., 20).\n",
    "4. Separately, fit 2 random forest classifiers with user and business features.\n",
    "5. For each set of business trajectories network recommendations, obtain the random forests' predictions to compare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 1: Get test users**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in reviews dataset to generate the business trajectories network\n",
    "reviews_df = pd.read_json(\"data/review.json\",lines=True)\n",
    "\n",
    "# get test users for business trajectories network evaluation: subset of users with above 30 visited businesses \n",
    "user_numinteractions = reviews_df.user_id.value_counts()\n",
    "users_above_30_interactions = user_numinteractions[user_numinteractions>30]\n",
    "test_users_above_30_ratings = list(users_above_30_interactions.sample(frac=0.3,random_state=1).index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 2: Build business trajectories network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for each user, get chronologically ordered list of positively reviewed businesses visited\n",
    "good_reviews_df = reviews_df[reviews_df['stars'] > 3]\n",
    "good_reviews_df.drop(columns = ['stars'],inplace=True)\n",
    "good_reviews_df = good_reviews_df.sort_values(by=['date']).reset_index(drop=True)\n",
    "business_visits_series = good_reviews_df.groupby('user_id').apply(lambda df: list(df['business_id']))\n",
    "business_visits_series = business_visits_series[business_visits_series.map(len)>1]\n",
    "business_visits_series_train = business_visits_series[~business_visits_series.index.isin(test_users_above_30_ratings)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate network\n",
    "\n",
    "source = []\n",
    "target = []\n",
    "for user_id, business_list in business_visits_series_train.iteritems():\n",
    "    for i,business in enumerate(business_list):\n",
    "        try:\n",
    "            target.append(business_list[i+1])\n",
    "            source.append(business)\n",
    "        except IndexError:\n",
    "            break\n",
    "\n",
    "raw_edges = pd.DataFrame(data = {'source':source,'target':target})\n",
    "weighted_edges = raw_edges.groupby(['source','target']).agg(len)\n",
    "weighted_edges = weighted_edges.to_frame().rename(columns={0:'weight'})\n",
    "weighted_edges = weighted_edges.reset_index()\n",
    "weighted_edges.to_csv(\"stash/business_network_edgelist.csv\")\n",
    "\n",
    "G = nx.from_pandas_edgelist(weighted_edges,'source','target',['weight'])\n",
    "\n",
    "weight_dict = defaultdict(int)\n",
    "for s,t,w in G.edges(data=True):\n",
    "    weight_dict[w['weight']] += w['weight']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 3: Generate business trajectories network recommendations for different values of `max direct neighbors`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate network recommendations for different number of DIRECT neighbors allowed in list\n",
    "\n",
    "def get_neighbors_from_node(node,graph) -> dict:\n",
    "    # given node and graph, returns dict of node neighbors and corresponding weights\n",
    "    return {neighbor:weight['weight'] for neighbor,weight in dict(graph[node]).items()}\n",
    "    \n",
    "\n",
    "def get_all_neighbors_from_nodes(nodes,graph) -> dict:\n",
    "    # given list of nodes, returns dict containing all neighbors of each node in nodes with corresponding weight\n",
    "    nodes_neighbors = []\n",
    "    for node in nodes:\n",
    "        nodes_neighbors.append(get_neighbors_from_node(node,graph))\n",
    "    return dict(itertools.chain.from_iterable(dct.items() for dct in nodes_neighbors))\n",
    "\n",
    "\n",
    "def top_20_businesses(fromNode,max_direct_neighbors,graph) -> list:\n",
    "    # Given business fromNode, this method returns the business network's top 20 business recommendations.\n",
    "    # Picks top max_direct_neighbors based on weight, then completes the rest of the list\n",
    "    # by fanning out to 2nd, 3rd, 4th...  degree neighbors and picking top businesses by rank.\n",
    "    \n",
    "    recommendations = []\n",
    "    \n",
    "    # get dataframe of direct neighbors and corresponding weights, sorted by weight \n",
    "    direct_neighbors_dict = get_all_neighbors_from_nodes(nodes=[fromNode], graph=graph)\n",
    "    direct_neighbors_df = pd.DataFrame.from_dict(direct_neighbors_dict,orient='index')\n",
    "    direct_neighbors_df.sort_values(by=0,ascending=False,inplace=True)\n",
    "    \n",
    "    # append at most max_direct_neighbors direct neighbors to recommendations\n",
    "    if len(direct_neighbors_df) <= max_direct_neighbors: \n",
    "        top_direct_neighbors = list(direct_neighbors_df.index)\n",
    "    elif len(direct_neighbors_df) > max_direct_neighbors: \n",
    "        top_direct_neighbors = list(direct_neighbors_df.iloc[0:max_direct_neighbors].index)\n",
    "    for direct_neighbor in top_direct_neighbors:\n",
    "        recommendations.append(direct_neighbor)\n",
    "\n",
    "    \n",
    "    # fan out to neighbors of neigbors by level to get top 20 - max_direct_neighbors remaining recommendations\n",
    "    while len(recommendations) < 20:\n",
    "        neighbors_by_level = list(direct_neighbors_df.index)\n",
    "        neighbors_of_neighbors_by_level_dict = get_all_neighbors_from_nodes(nodes=neighbors_by_level, graph=graph)\n",
    "        neighbors_of_neighbors_by_level_df = pd.DataFrame.from_dict(neighbors_of_neighbors_by_level_dict,orient='index')\n",
    "        neighbors_of_neighbors_by_level_df.sort_values(by=0,ascending=False,inplace=True)\n",
    "\n",
    "        if len(neighbors_of_neighbors_by_level_df) >= 20 - len(recommendations):\n",
    "            top_neighbors_of_neighbors_by_level = list(neighbors_of_neighbors_by_level_df.iloc[0:20-len(recommendations)].index)\n",
    "        else:\n",
    "            top_neighbors_of_neighbors_by_level = list(neighbors_of_neighbors_by_level_df.index)\n",
    "        for neighbor in top_neighbors_of_neighbors_by_level:\n",
    "            recommendations.append(neighbor)\n",
    "        neighbors_by_level = list(neighbors_of_neighbors_by_level_df.index)\n",
    "    return recommendations\n",
    "\n",
    "\n",
    "def generate_network_recommendations(max_direct_neighbors, business_visits_series, graph)-> pd.DataFrame:\n",
    "    # for each user, generate top 20 businesses to recommend given first valid business visited (has to be in network)\n",
    "    user_dataframes = []\n",
    "    \n",
    "    print(f'number of users {len(business_visits_series)}')\n",
    "    denom = len(business_visits_series)\n",
    "    count = 0\n",
    "    for user, businesses in business_visits_series.iteritems():\n",
    "        if count % 1000 == 0:\n",
    "            print(f'progress: {count/denom *100}%')\n",
    "            print(f'users completed {count} out of {denom}')\n",
    "        if businesses[0] in graph.nodes:\n",
    "            first_valid_business = businesses[0]\n",
    "        else:\n",
    "            for j,business in enumerate(businesses):\n",
    "                if business in graph.nodes:\n",
    "                    first_valid_business = business\n",
    "                    break\n",
    "        recommendations = top_20_businesses(first_valid_business, max_direct_neighbors, graph)\n",
    "        user_recommendations_df = pd.DataFrame({\n",
    "                                                'user': [user for _ in range(20)],\n",
    "                                                'recommendations': recommendations,\n",
    "                                                'network_rank': [i+1 for i in range(20)],\n",
    "                                                'business_net_pred': [1 for _ in range(20)]\n",
    "                                                })\n",
    "\n",
    "        user_dataframes.append(user_recommendations_df)\n",
    "        count += 1\n",
    "    all_user_network_recommendations = pd.concat(user_dataframes).reset_index().drop(columns=['index'])\n",
    "    all_user_network_recommendations.to_csv(f'stash/network_recommendations/network_recommendations_{max_direct_neighbors}_max_direct_neighbors.csv')\n",
    "    return all_user_network_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get test users\n",
    "business_visits_series_test = business_visits_series[business_visits_series.index.isin(test_users_above_30_ratings)]\n",
    "recommendations_dict = dict()\n",
    "\n",
    "# generate network recommendations with different max_direct_neighbors\n",
    "for i in range(20):\n",
    "    print(f'Generating recommendations for test users with constraint {i+1} max direct neighbors')\n",
    "    recommendations = generate_network_recommendations(max_direct_neighbors = i+1,\n",
    "                                                       business_visits_series = business_visits_series_test, \n",
    "                                                       graph = G)\n",
    "    recommendations_dict[i+1] = recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 4: Build random forest classifiers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/john/.virtualenvs/fdy/lib/python3.6/site-packages/numpy/lib/arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "# read in training data\n",
    "data = pd.read_csv(\"data/training_data.csv\",index_col=0)\n",
    "\n",
    "# get users who have rated over 30 businesses\n",
    "groups = data.groupby('user_id')\n",
    "group_df = groups.nunique()\n",
    "group_df_filtered = group_df.loc[group_df[\"business_id\"]>30][[\"business_id\", \"stars\"]]\n",
    "group_df_filtered.rename(columns={\"business_id\":\"num_businesses_rated\"}, inplace=True)\n",
    "\n",
    "# training data with users who have rated over 30 businesses\n",
    "data_process = data.join(group_df_filtered.drop(columns = ['stars']), on=\"user_id\", how=\"inner\")\n",
    "data_process.drop(columns = ['business_longitude','business_latitude','business_review_count','user_first_elite_year', \n",
    "                             'user_last_elite_year','user_begin_yelping_year', 'user_begin_yelping_month',\n",
    "                             'user_begin_yelping_day','num_businesses_rated'],inplace=True)\n",
    "\n",
    "data_process = data_process[~data_process['user_id'].isin(test_users_above_30_ratings)]\n",
    "\n",
    "training_data_with_net_info = data_process.drop(columns=['business_id','user_id'])\n",
    "training_data_no_net_info = data_process.drop(columns=['business_id','user_id','business_latent_category'])\n",
    "\n",
    "# training data with business categories feature\n",
    "y = training_data_with_net_info['stars'].values\n",
    "X = training_data_with_net_info.drop(columns=['stars']).values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.30,\n",
    "                                                    random_state=101)\n",
    "# training data without business categories feature\n",
    "y_noNet = training_data_no_net_info['stars'].values\n",
    "X_noNet = training_data_no_net_info.drop(columns=['stars']).values\n",
    "X_noNet_train, X_noNet_test, y_noNet_train, y_noNet_test = train_test_split(X_noNet, y_noNet,\n",
    "                                                                            test_size=0.30,\n",
    "                                                                            random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_net = RandomForestClassifier(n_estimators=100)\n",
    "rfc_net.fit(X_train, y_train)\n",
    "file1 = 'stash/random_forest/rfc_net.pkl'\n",
    "rfc_net = pickle.load(open(file1, 'rb'))\n",
    "with open(\"stash/random_forest/rfc_net.pkl\", 'wb') as file:\n",
    "    pickle.dump(rfc_net, file)\n",
    "\n",
    "rfc_NoNet = RandomForestClassifier(n_estimators=100)\n",
    "rfc_NoNet.fit(X_noNet_train, y_noNet_train)\n",
    "file2 = 'stash/random_forest/rfc_noNet.pkl'\n",
    "rfc_NoNet = pickle.load(open(file2, 'rb'))\n",
    "with open(\"stash/random_forest/rfc_noNet.pkl\", 'wb') as file:\n",
    "    pickle.dump(rfc_NoNet, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 97888  38465]\n",
      " [ 21577 240678]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.72      0.77    136353\n",
      "           1       0.86      0.92      0.89    262255\n",
      "\n",
      "    accuracy                           0.85    398608\n",
      "   macro avg       0.84      0.82      0.83    398608\n",
      "weighted avg       0.85      0.85      0.85    398608\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rfc_net_pred = rfc_net.predict(X_test)\n",
    "print(confusion_matrix(y_test,rfc_net_pred))\n",
    "print(classification_report(y_test,rfc_net_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 96616  39737]\n",
      " [ 22732 239523]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.71      0.76    136353\n",
      "           1       0.86      0.91      0.88    262255\n",
      "\n",
      "    accuracy                           0.84    398608\n",
      "   macro avg       0.83      0.81      0.82    398608\n",
      "weighted avg       0.84      0.84      0.84    398608\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rfc_NoNet_pred = rfc_NoNet.predict(X_noNet_test)\n",
    "print(confusion_matrix(y_noNet_test, rfc_NoNet_pred))\n",
    "print(classification_report(y_noNet_test, rfc_NoNet_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 5: Get random forest predictions of each set of business trajectories network's recommendations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get random forest predictions for the set of recommendations from the business trajectories network\n",
    "\n",
    "\n",
    "# defining order on columns for training data\n",
    "business_cols = ['business_id', 'business_stars', 'business_state', 'business_latent_category',\n",
    "                 'business_days_open_weekly', 'average_open_time', 'average_close_time','business_city']\n",
    "                 \n",
    "user_cols = ['user_id', 'user_average_stars', 'user_compliment_cool', 'user_compliment_cute',\n",
    "             'user_compliment_funny', 'user_compliment_hot', 'user_compliment_list',\n",
    "             'user_compliment_more', 'user_compliment_note',\n",
    "             'user_compliment_photos', 'user_compliment_plain',\n",
    "             'user_compliment_profile', 'user_compliment_writer', 'user_cool',\n",
    "             'user_fans', 'user_funny', 'user_review_count', 'user_useful',\n",
    "             'user_num_elite_years', 'user_friends_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get test users\n",
    "# training data with users who have rated over 30 businesses\n",
    "data_process_net_test = data[data['user_id'].isin(test_users_above_30_ratings)]\n",
    "user_training_data = data[user_cols].drop_duplicates()\n",
    "business_training_data = data[business_cols].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each set of recommendations (max_direct_neighbors), get random forest predictions for each\n",
    "# user business (recommendation) pair\n",
    "\n",
    "# get all businesses each user liked; this will be useful for finding network recommendation matches\n",
    "user_visited_businesses_df = good_reviews_df[['user_id','business_id']].groupby('user_id').agg(set)\n",
    "user_visited_businesses_df = user_visited_businesses_df.reset_index()\n",
    "user_visited_businesses_map = dict()\n",
    "for item, row in user_visited_businesses_df.iterrows():\n",
    "    user = row.user_id\n",
    "    businesses_set = row.business_id\n",
    "    user_visited_businesses_map[user] = businesses_set\n",
    "\n",
    "for i in range(20):\n",
    "    # for each user business pair in recommendations, get corresponding user business metadata\n",
    "    print(f'{i+1} max direct neighbors')\n",
    "    recommendations = recommendations_dict[i+1]\n",
    "    business_join = recommendations.join(business_training_data.set_index('business_id'),on='recommendations')\n",
    "    user_join = business_join.join(user_training_data.set_index('user_id'),on='user')\n",
    "    network_recommendations_data = user_join.dropna()\n",
    "    network_recommendations_data = network_recommendations_data.reset_index().drop(columns = ['index'])\n",
    "    \n",
    "    # get appropriate subset of columns needed for random forest predictions\n",
    "    training_data_with_net_info_cols = list(training_data_with_net_info.columns)\n",
    "    training_data_with_net_info_cols.remove('stars')\n",
    "    training_data_no_net_info_cols = list(training_data_no_net_info.columns)\n",
    "    training_data_no_net_info_cols.remove('stars')  \n",
    "    net_test_with_lat_cat = network_recommendations_data[training_data_with_net_info_cols]\n",
    "    net_test_no_lat_cat = network_recommendations_data[training_data_no_net_info_cols]\n",
    "    \n",
    "    # random forest predictions for all user business pairs in business trajectories' network recommendations\n",
    "    rfc_net_recommend_pred = rfc_net.predict(net_test_with_lat_cat.values)\n",
    "    rfc_net_recommend_pred_proba = rfc_net.predict_proba(net_test_with_lat_cat.values)\n",
    "    rfc_NoNet_recommend_pred = rfc_NoNet.predict(net_test_no_lat_cat.values)\n",
    "    rfc_NoNet_recommend_pred_proba = rfc_NoNet.predict_proba(net_test_no_lat_cat.values) \n",
    "    rf_net_pred_df = pd.DataFrame({'rf_net_pred':rfc_net_recommend_pred})\n",
    "    rf_net_prob_like_df = pd.DataFrame({'rf_net_prob_like':rfc_net_recommend_pred_proba[:,1]}) \n",
    "    rf_NoNet_pred_df = pd.DataFrame({'rf_NoNet_pred':rfc_NoNet_recommend_pred})\n",
    "    rf_NoNet_prob_like_df = pd.DataFrame({'rf_NoNet_prob_like':rfc_NoNet_recommend_pred_proba[:,1]}) \n",
    "    \n",
    "    # merge random forest predictions back to recommendations dataframe\n",
    "    narrow_recs = network_recommendations_data[['user','recommendations','network_rank','business_net_pred']]\n",
    "    df1 = pd.merge(narrow_recs,rf_net_pred_df,left_index=True, right_index=True)\n",
    "    df2 = pd.merge(df1,rf_net_prob_like_df,left_index=True, right_index=True)\n",
    "    df3 = pd.merge(df2,rf_NoNet_pred_df,left_index=True, right_index=True)\n",
    "    recommendations = pd.merge(df3,rf_NoNet_prob_like_df,left_index=True, right_index=True)\n",
    "        \n",
    "    # append true_like column to recommendations \n",
    "    recommendations['true_like'] = recommendations.apply(lambda row: 1 if row['recommendations'] in user_visited_businesses_map[row['user']] else 0, axis = 1)\n",
    "    recommendations.to_csv(f'stash/network_recommendations_with_rf_predictions/network_recommendations_with_rf_predictions_{i+1}_max_direct_neighbors.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
